{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'courseRecSys.xlsx'\n",
    "splitProp = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    df = pd.read_excel(filename,\n",
    "    header=0,\n",
    "    index_col=False,\n",
    "    keep_default_na=True\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df=df.drop(['Start time','Completion time','Email','Name','Name(not mandatory)\\n','ID'], axis=1)\n",
    "    \n",
    "    df[\"avg\"] = df.mean(axis = 0, skipna=True, numeric_only=True)\n",
    "    df=df.apply(lambda x: x.fillna(x.mean()),axis=0)\n",
    "    df = df.drop([\"avg\"],axis =1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=read_data(filename)\n",
    "df = preprocess(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(df,train_size=splitProp, random_state=42,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses = df.columns\n",
    "courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_courses = ['Data Structures and Algorithms','Computer Architecture\\n','Discrete Mathematics\\n','Economics','Programming-2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(point):\n",
    "    wcss = []  #Within Cluster Sum of Squares\n",
    "    for number_of_clusters in range(1, 15): \n",
    "        kmeans = KMeans(n_clusters = number_of_clusters, random_state = 42)\n",
    "        kmeans.fit(df[core_courses]) \n",
    "        wcss.append(kmeans.inertia_)\n",
    "    ks = np.arange(1,15,1)\n",
    "    plt.plot(ks, wcss)\n",
    "    plt.axvline(point, linestyle='--', color='r')\n",
    "    return wcss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_means(df, num_clusters, iters):\n",
    "    np_dat = df.to_numpy()\n",
    "    # Initialize the centroids\n",
    "    range_min = np.min(np_dat, axis = 0)\n",
    "    range_max = np.max(np_dat, axis = 0)\n",
    "    curr_centroids = []\n",
    "    np.random.seed(75)\n",
    "    for i in range(num_clusters):\n",
    "        curr_centroids.append(np.random.uniform(range_min, range_max))\n",
    "    clusters = {}\n",
    "    for it in range(iters):\n",
    "        # Repopulate clusters\n",
    "        clusters.clear()\n",
    "        for j in range(np_dat.shape[0]):\n",
    "            datapoint = np_dat[j]\n",
    "            min = sys.maxsize\n",
    "            min_index = 10\n",
    "            for k in range(num_clusters):\n",
    "                dist = np.linalg.norm(curr_centroids[k] - datapoint)\n",
    "                if(dist < min):\n",
    "                    min = dist\n",
    "                    min_index = k\n",
    "            if(clusters.get(str(min_index)) == None):\n",
    "                clusters[str(min_index)] = []\n",
    "            clusters[str(min_index)].append(j)\n",
    "        \n",
    "        # Recalculate Centroids\n",
    "        new_centroids = []\n",
    "        for k in range(num_clusters):\n",
    "            key = str(k)\n",
    "            if(clusters.get(key) == None): \n",
    "                n_centr = curr_centroids[k]\n",
    "                #n_centr = np.random.uniform(range_min, range_max)\n",
    "            else: n_centr = np.mean(np_dat[np.array(clusters[key])], axis = 0)\n",
    "            new_centroids.append(n_centr)\n",
    "        \n",
    "        #if((np.not_equal(new_centroids, curr_centroids).any()) != True ): break\n",
    "        curr_centroids = new_centroids\n",
    "        #print(it)\n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clustered_df(df,num_clusters):\n",
    "   \n",
    "    cluster_dict = K_means(df[core_courses], num_clusters, 50)\n",
    "    print(cluster_dict)\n",
    "    clustered_df =[]\n",
    "    arr = df.to_numpy()\n",
    "    for k in range(num_clusters):\n",
    "        key = str(k)\n",
    "        centr = np.nanmean(arr[np.array(cluster_dict[key])], axis = 0)\n",
    "        clustered_df.append(centr)\n",
    "    \n",
    "    return clustered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_get_clustered_df(df,num_clusters):\n",
    "    kmeans = KMeans(n_clusters = num_clusters, random_state = 42)\n",
    "    clustered_matrix = kmeans.fit_predict(df[core_courses]) \n",
    "    print(clustered_matrix.shape)\n",
    "    arr = df.to_numpy()\n",
    "    clustered_df = []\n",
    "    for i in range(num_clusters):\n",
    "        users = 0\n",
    "        s=np.zeros(25)\n",
    "        for j in range(len(clustered_matrix)):#162\n",
    "            \n",
    "            if clustered_matrix[j]==i:\n",
    "                s+=arr[j]\n",
    "                users = users + 1\n",
    "        mean = s/users\n",
    "        clustered_df.append(mean)\n",
    "    return clustered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_df = old_get_clustered_df(df,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd(A):\n",
    "    \n",
    "    # Step 1: Compute the transpose of A\n",
    "    AT = A.T\n",
    "    # Step 2: Compute the product A x AT\n",
    "    ATA = A.dot(AT)\n",
    "    # Step 3: Compute the eigenvalues and eigenvectors of ATA\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(ATA)\n",
    "    # Step 4: Sort the eigenvalues in descending order\n",
    "    sorted_idx = eigenvalues.argsort()[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_idx]\n",
    "    eigenvectors = eigenvectors[:, sorted_idx]\n",
    "    # Step 5: Compute the diagonal matrix Sigma\n",
    "    Sigma = np.sqrt(eigenvalues)\n",
    "    # Step 6: Compute the pseudo-inverse of Sigma\n",
    "    inv_Sigma = np.zeros((A.shape[1], A.shape[0]))\n",
    "    inv_Sigma[:A.shape[0], :A.shape[0]] = np.diag(1.0 / Sigma)\n",
    "    # Step 7: Compute the product AT x V x inv(Sigma)\n",
    "    V = AT.dot(eigenvectors)\n",
    "    # Step 8: Normalize the columns of V\n",
    "    V = V / np.linalg.norm(V, axis=0)\n",
    "    # Step 9: Compute the product A x U x Sigma^-1\n",
    "    U = A.dot(V) / Sigma\n",
    "    return U, Sigma, V.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U,sigma,VT = svd(np.array(clustered_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.shape,sigma.shape,VT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduced_svd(k,A):\n",
    "    U,sigma,VT = svd(A)\n",
    "    reduced_matrix = np.dot(np.dot(U[:,:k],np.diag(sigma[:k])),VT[:k,:])\n",
    "    return reduced_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_matrix = reduced_svd(k,np.array(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EuclDist(vec1,vec2):\n",
    "    return np.linalg.norm(vec1 - vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user = [0.2,0.8,0.2,0.6,0.9]\n",
    "new_user2 =[ 0.699459,\t0.545405\t,0.325405\t,0.615135\t,0.737838]\n",
    "sooraj = [0.4, 0.8,0.85,0.9,0.7]\n",
    "chinmay = [0.9,0.2,0.4,0.6 ,0.8]\n",
    "kritin = [0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_cluster(reduced_matrix,new_user):\n",
    "    \n",
    "    min=EuclDist(reduced_matrix[0,:5],new_user)\n",
    "    cluster = 0\n",
    "    for i in range(len(reduced_matrix)):\n",
    "        sim = EuclDist(reduced_matrix[i,:5],new_user)#euclidean distance\n",
    "        # print(sim)\n",
    "        if sim<min:\n",
    "            min = sim\n",
    "            cluster=i\n",
    "    return cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_cluster(reduced_matrix,kritin)#testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_elective_ratings(reduced_matrix,new_user):\n",
    "    cluster = find_cluster(reduced_matrix,new_user)\n",
    "    elective_ratings = reduced_matrix[cluster,5:]\n",
    "    return elective_ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_elective_ratings_from_og(reduced_matrix,original,new_user):\n",
    "    cluster = find_cluster(reduced_matrix,new_user)\n",
    "    elective_ratings = original[cluster,5:]\n",
    "    return elective_ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elective_ratings = find_elective_ratings(reduced_matrix,kritin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elective_ratings2 = find_elective_ratings_from_og(reduced_matrix,np.array(clustered_df),kritin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ranking of electives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_electives_ranked(courses,elective_ratings):\n",
    "    d={courses[5:][i]: elective_ratings[i] for i in range(20)}\n",
    "\n",
    "    elective_ranked = sorted(courses[5:], key=lambda x : -d[x])\n",
    "    return elective_ranked  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elective_ranked = get_electives_ranked(courses,elective_ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elective_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elective_ranked2 = get_electives_ranked(courses,elective_ratings2)\n",
    "elective_ranked2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change the nan values to average of the item instead of average of user ratings. -- done\n",
    "\n",
    "in find elective rating find the ratings from the clustered matrix and not reduced matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_recommendations(X_train,num_clusters):\n",
    "    clustered_df=get_clustered_df(X_train,num_clusters)\n",
    "    reduced_matrix=reduced_svd(6,np.array(clustered_df))\n",
    "    return reduced_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_train_recommendations(X_train,num_clusters):\n",
    "    clustered_df=old_get_clustered_df(X_train,num_clusters)\n",
    "    reduced_matrix=reduced_svd(6,np.array(clustered_df))\n",
    "    return reduced_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_recommendations(X_test,reduced_matrix):\n",
    "    pred = []\n",
    "    for i in range(len(X_test)):\n",
    "        new_user = X_test[i]\n",
    "        elective_ratings = find_elective_ratings(reduced_matrix,new_user[:5])\n",
    "        elective_ranked = get_electives_ranked(courses,elective_ratings)\n",
    "        pred.append(elective_ratings)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_matrix = train_recommendations(train,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=test_recommendations(test.to_numpy(),reduced_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred,test):\n",
    "    mse =[]\n",
    "    for i in range(len(pred)):\n",
    "        mse.append((EuclDist(pred[i],test[i][5:])))\n",
    "\n",
    "    return sum(mse)/len(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(pred,test.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pred:\n",
    "    elective_ranked = get_electives_ranked(courses,i)\n",
    "    print(elective_ranked)\n",
    "    print(\"*****************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "X_std = StandardScaler().fit_transform(df)\n",
    "pca = PCA(n_components=4)\n",
    "principalComponents = pca.fit_transform(X_std)\n",
    "# Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_ratio_, color='black')\n",
    "plt.xlabel('PCA features')\n",
    "plt.ylabel('variance %')\n",
    "plt.xticks(features)\n",
    "# Save components to a DataFrame\n",
    "PCA_components = pd.DataFrame(principalComponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(PCA_components[1], PCA_components[2], alpha=.5, color='black')\n",
    "plt.xlabel('PCA 2')\n",
    "plt.ylabel('PCA 3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, 10)\n",
    "inertias = []\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(PCA_components.iloc[:,:3])\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "plt.plot(ks, inertias, '-o', color='black')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
